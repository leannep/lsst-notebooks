{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65558f55-5741-43c9-8c89-49c7ad09a660",
   "metadata": {},
   "source": [
    "Working with the parquest files to do a full analysis of the full DP0 dataset - no subsetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4062243e-4828-4188-bf97-54c93739986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from astropy.units import UnitsWarning\n",
    "import re\n",
    "\n",
    "# LSST packages\n",
    "from lsst.daf.butler import Butler\n",
    "from lsst.rsp import get_tap_service\n",
    "\n",
    "# Astropy\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.units.quantity import Quantity\n",
    "from astropy.visualization import ZScaleInterval, AsinhStretch\n",
    "\n",
    "# Bokeh\n",
    "import bokeh\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.models import ColumnDataSource, Range1d, HoverTool\n",
    "from bokeh.models import CDSView, GroupFilter\n",
    "from bokeh.plotting import figure, gridplot\n",
    "from bokeh.transform import factor_cmap\n",
    "\n",
    "# HoloViews\n",
    "import holoviews as hv\n",
    "from holoviews import streams, opts\n",
    "from holoviews.operation.datashader import datashade, dynspread, rasterize\n",
    "from holoviews.plotting.util import process_cmap\n",
    "\n",
    "# Datashader\n",
    "import datashader as ds\n",
    "import colorcet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc148f-bc9d-4d2e-b892-958829e23d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  warnings\n",
    "warnings.filterwarnings(\"ignore\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c26660-4caa-48c4-a4bf-3ed65362e978",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 5)\n",
    "hv.extension('bokeh')\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5146fda-2cde-410a-b5ca-a671445d71f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAP service \n",
    "service = get_tap_service()\n",
    "assert service is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e24599-8e9c-46c8-8050-7875a7471709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# 157 tracts were processed by DP0.2\n",
    "# query = \"\"\"\n",
    "# SELECT distinct tract\n",
    "# FROM dp02_dc2_catalogs.Object \n",
    "# \"\"\"\n",
    "# print(query)\n",
    "\n",
    "## select tract from  GROUP BY tract having count(*) >= 2 \n",
    "#job = service.submit_job(query)\n",
    "#job.run()\n",
    "#job.wait(phases=['COMPLETED', 'ERROR'])\n",
    "#print('Job phase is', job.phase)\n",
    "\n",
    "# data = job.fetch_result().to_table().to_pandas()\n",
    "# data\n",
    "#assert len(data) == 157\n",
    "\n",
    "# data = data.sort_values(by = \"tract\")\n",
    "# data.to_csv(\"./data/dp0-2_tracts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e397d7b-7716-456d-9383-eb6c21ae0caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Butler\n",
    "config = 'dp02'\n",
    "collection = '2.2i/runs/DP0.2'\n",
    "butler = Butler(config, collections=collection)\n",
    "registry = butler.registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4cb85b-a3c4-492d-8ef6-b5fd36c5027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What parquet tables exist in the DP0.2 dataset \n",
    "pattern = re.compile('^.*Table.*$')\n",
    "for dst in sorted(registry.queryDatasetTypes(pattern)):\n",
    "    print(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e39042-bc4d-4a1a-bd88-61dc2cd56af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a pandas dataframe from the parquet file for a tract and a patch - note that all the columns are returned \n",
    "butler.get(\"objectTable\", dataId={\"tract\": 4431, \"patch\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edbc1cb-93a7-4d4c-bf4f-ea2e89b1dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now specify just the ra and dec columns \n",
    "butler.get(\"objectTable\", dataId={\"tract\": 4431, \"patch\": 5}, \n",
    "           parameters={\"columns\": [ \"coord_ra\", \"coord_dec\",\n",
    "                                   \"g_cModelFlux\", \"r_cModelFlux\", \"i_cModelFlux\"]}\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9321d800-902c-4293-bbe5-56e42a35e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now from the objectTable_tract # 4431\n",
    "da = butler.get(\"objectTable_tract\", dataId={\"tract\": 4431}, \n",
    "                parameters={\"columns\": [ \"coord_ra\", \"coord_dec\",\n",
    "                                        \"g_cModelFlux\", \"r_cModelFlux\", \"i_cModelFlux\"]}\n",
    "               )\n",
    "len(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ebed7b-953c-4c56-b40a-62bc35cd6e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tract_id in [4432, 4433, 4434, 4435, 4436]:\n",
    "    dataId = dict(tract=tract_id)\n",
    "    print(dataId)\n",
    "    df = butler.get(\"objectTable_tract\", dataId=dataId, \n",
    "                    parameters={\"columns\": [ \"coord_ra\", \"coord_dec\",\n",
    "                                            \"g_cModelFlux\", \"r_cModelFlux\", \"i_cModelFlux\"]}\n",
    "                   )\n",
    "    da = pd.concat([da, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ab44d9-f15e-4f31-a030-c34a0d1a75cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert to mag\n",
    "# Converts a calibrated (AB) flux to an AB magnitude.\n",
    "def nanojanskyToABMagnitude(flux):\n",
    "\n",
    "    # The Oke & Gunn (1983) AB magnitude reference flux, in nJy (often approximated as 3631.0).\n",
    "    referenceFlux = 1e23 * np.power(10, (48.6 / -2.5)) * 1e9\n",
    "    abMag = -2.5 * np.log10(flux / referenceFlux)\n",
    "    return abMag\n",
    "\n",
    "assert nanojanskyToABMagnitude(54.3409121) == 27.062182690804125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f41d4a-655e-49c6-89a1-b4f041950407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to ABMag and compute colors\n",
    "da['mag_g_cModel'] = nanojanskyToABMagnitude(da['g_cModelFlux'])\n",
    "da['mag_r_cModel'] = nanojanskyToABMagnitude(da['r_cModelFlux'])\n",
    "da['mag_i_cModel'] = nanojanskyToABMagnitude(da['i_cModelFlux'])               \n",
    "da['gmi'] = da['mag_g_cModel'] - da['mag_i_cModel']\n",
    "da['rmi'] = da['mag_r_cModel'] - da['mag_i_cModel']\n",
    "da['gmr'] = da['mag_g_cModel'] - da['mag_r_cModel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7ecc4f-acac-4078-804e-bfe9c3c0cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# cvs = ds.Canvas(plot_width=850, plot_height=500)\n",
    "# agg = cvs.points(da, 'coord_ra', 'coord_dec')\n",
    "# img = ds.tf.shade(agg, cmap=colorcet.blues, how='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fdfbdc-4092-440d-a0f7-53e129993e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e95e939-2d8d-4d62-a836-36f316747158",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "points = hv.Points((da['gmr'], da['rmi']))\n",
    "boundsxy = (0, 0, 0, 0)\n",
    "box = streams.BoundsXY(source=points, bounds=boundsxy)\n",
    "bounds = hv.DynamicMap(lambda bounds: hv.Bounds(bounds), streams=[box])\n",
    "p = dynspread(datashade(points, cmap=\"Viridis\"))\n",
    "p = p.opts(width=800, height=300, padding=0.05, show_grid=True,\n",
    "           xlim=(-2.0, 7.0), ylim=(-5.0, 3.0), xlabel=\"(g-r)\", ylabel=\"(r-i)\",\n",
    "           tools=['box_select', 'lasso_select'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9a00b1-a6e1-4fe9-b23e-8f05a3400e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "p * bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d3941-fc57-45c7-aff6-1fc2c034affc",
   "metadata": {},
   "source": [
    "We currently have  have columnar access, but no row filtering - so we cannot filter by \"columnX\" = True. TAP/Q+ADQL is better designed for this sort of access pattern. Parquet provides a columnar format so row filtering in a memory efficient way is non trivial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf087d55-36d7-4ddd-a4c1-5318784276df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the forcedSourceTable but not all the columns - this will take forever and probably crash. \n",
    "# There are per-detector and per-patch parquet tables.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f670e88-9fbd-4fec-936a-6f3bccfdb3f0",
   "metadata": {},
   "source": [
    "# Some notes on using datashader\n",
    "Q: When should I use datashader?\n",
    "\n",
    "A: Datashader is designed for working with large datasets, for cases where it is most crucial to faithfully represent\n",
    "the distribution of your data. datashader can work easily with extremely large datasets, generating a fixed-size data\n",
    "structure (regardless of the original number of records) that gets transferred to your local browser for display. If you\n",
    "ever find yourself subsampling your data just so that you can plot it feasibly, or if you are forced for practical reasons\n",
    "to iterate over chunks of it rather than looking at all of it at once, then datashader can probably help you.\n",
    "\n",
    "\n",
    "Q: When should I not use datashader?\n",
    "\n",
    "A: If you have a very small number of data points (in the hundreds or thousands) or curves (in the tens or several tens,\n",
    "each with hundreds or thousands of points), then conventional plotting packages like Bokeh may be more suitable.\n",
    "With conventional browser-based packages, all of the data points are passed directly to the browser for display, allowing specific interaction with each curve or point, including display of metadata, linking to sources, etc. This approach\n",
    "offers the most flexibility per point or per curve, but rapidly runs into limitations on how much data can be processed\n",
    "by the browser, and how much can be displayed on screen and resolved by the human visual system. If you are not\n",
    "having such problems, i.e., your data is easily handled by your plotting infrastructure and you can easily see and work\n",
    "with all your data onscreen already, then you probably donâ€™t need datashader.\n",
    "\n",
    "\n",
    "Q: Is datashader part of bokeh?\n",
    "\n",
    "A: datashader is an independent project, focusing on generating aggregate arrays and representations of them as\n",
    "images. Bokeh is a complementary project, focusing on building browser-based visualizations and dashboards. Bokeh\n",
    "(along with other plotting packages) can display images rendered by datashader, providing axes, interactive zooming\n",
    "and panning, selection, legends, hover information, and so on. Sample bokeh-based plotting code is provided with\n",
    "datashader, but viewers for maptlotlib are already under development, and similar code could be developed for any\n",
    "other plotting package that can display images. The library can also be used separately, without any external plotting\n",
    "packages, generating images that can be displayed directly or saved to disk, or generating aggregate arrays suitable for\n",
    "further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d90f9c3-bcb2-427b-8d0e-992873086829",
   "metadata": {},
   "source": [
    "PyArrow\n",
    "\n",
    "Apache Arrow defines itself as a language-independent columnar memory format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b798600c-7871-4c33-a19a-b98759d4f49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72182567-a4c8-4bc0-b1ac-61b81d77e6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a PyArrow table \n",
    "da = pa.Table.from_pandas(df)\n",
    "len(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f3d248-27fa-4dcf-bf3f-9559937a021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8af8591-7c7a-4733-9245-a8c70009d05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "daa = pa.concat_tables([da1,da2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
